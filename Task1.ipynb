{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXXllnmzfsut",
        "outputId": "5f63c850-9bfc-4120-fc48-4cd67955cd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import string\n",
        "from pyngrok import ngrok\n",
        "import re\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Word Count\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "stop_words = set([\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
        "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
        "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
        "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
        "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
        "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\n",
        "    \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\",\n",
        "    \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
        "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
        "    \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\",\n",
        "    \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
        "])\n",
        "\n",
        "def clean_word(word):\n",
        "    return re.sub(r'[^\\w\\s]', '', word).lower()\n",
        "\n",
        "files = ['/content/input1.txt', '/content/input2.txt']\n",
        "rdd = spark.sparkContext.textFile(files[1]).union(spark.sparkContext.textFile(files[0]))\n",
        "\n",
        "uncleaned_word_counts = (\n",
        "    rdd.flatMap(lambda line: line.split())\n",
        "       .map(lambda word: (word, 1))\n",
        "       .reduceByKey(lambda a, b: a + b)\n",
        "       .sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "uncleaned_output = uncleaned_word_counts.collect()\n",
        "with open('output_1.txt', 'w') as f:\n",
        "    for word, count in uncleaned_output:\n",
        "        f.write(f\"{word}: {count}\\n\")\n",
        "\n",
        "word_counts = (\n",
        "    rdd.flatMap(lambda line: line.split())\n",
        "       .map(clean_word)\n",
        "       .filter(lambda word: word not in stop_words)\n",
        "       .map(lambda word: (word, 1))\n",
        "       .reduceByKey(lambda a, b: a + b)\n",
        "       .sortBy(lambda x: x[1], ascending=False)\n",
        ")\n",
        "\n",
        "output = word_counts.collect()\n",
        "with open('output_3.txt', 'w') as f:\n",
        "    for word, count in output:\n",
        "        f.write(f\"{word}: {count}\\n\")\n",
        "\n",
        "print(\"Top words:\")\n",
        "for word, count in output[:25]:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "ngrok.set_auth_token(\"2oY32y1o2wSGEJe5t6a6WZAnWxW_7cMg48xpKAosLShJ5LDdX\")\n",
        "public_url = ngrok.connect(4040)\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XPc_8Tof8--",
        "outputId": "f0e2804c-1146-4ad5-a5cb-ba65d07d4575"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words:\n",
            "mr: 889\n",
            "said: 638\n",
            "elizabeth: 603\n",
            "could: 593\n",
            "would: 532\n",
            "one: 425\n",
            "mrs: 387\n",
            "darcy: 378\n",
            "much: 373\n",
            "must: 364\n",
            "miss: 353\n",
            "know: 339\n",
            "bennet: 307\n",
            "little: 293\n",
            "never: 292\n",
            "think: 286\n",
            "time: 281\n",
            "well: 281\n",
            "jane: 271\n",
            "though: 267\n",
            "bingley: 261\n",
            "man: 253\n",
            "see: 240\n",
            "may: 237\n",
            "might: 232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-12T00:20:06+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://7778-34-105-21-235.ngrok-free.app\" -> \"http://localhost:4040\"\n"
          ]
        }
      ]
    }
  ]
}