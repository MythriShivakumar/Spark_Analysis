{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THHU13mogQA5",
        "outputId": "a767d971-da91-40a9-8a88-a13c7daf6a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyngrok import ngrok\n",
        "spark = SparkSession.builder.appName(\"DijkstraShortestPath\").getOrCreate()\n",
        "file1 = spark.read.csv(\"question2_1.txt\", sep=\",\", inferSchema=True, header=False).toDF(\"source\", \"destination\", \"weight\")\n",
        "file2 = spark.read.csv(\"question2_2.txt\", sep=\",\", inferSchema=True, header=False).toDF(\"source\", \"destination\", \"weight\")\n",
        "edges = file1.union(file2)\n",
        "\n",
        "edges_rdd = edges.rdd.map(lambda row: (row[\"source\"], (row[\"destination\"], row[\"weight\"])))\n",
        "\n",
        "start_node = 0\n",
        "nodes = edges.select(\"source\").distinct().union(edges.select(\"destination\").distinct()).distinct()\n",
        "distances = {row[0]: float('inf') for row in nodes.collect()}\n",
        "distances[start_node] = 0\n",
        "distances_broadcast = spark.sparkContext.broadcast(distances)\n",
        "for i in range(len(distances)):\n",
        "    new_distances = (\n",
        "        edges_rdd.flatMap(lambda edge: [(edge[0], distances_broadcast.value[edge[0]]),\n",
        "                                        (edge[1][0], distances_broadcast.value[edge[0]] + edge[1][1])])\n",
        "        .reduceByKey(min)\n",
        "    )\n",
        "    new_distances_dict = dict(new_distances.collect())\n",
        "    for node, dist in new_distances_dict.items():\n",
        "        distances[node] = dist if dist < distances[node] else distances[node]\n",
        "    distances_broadcast = spark.sparkContext.broadcast(distances)\n",
        "distances = {node: float(dist) for node, dist in distances.items()}\n",
        "distances_df = spark.createDataFrame([Row(node=node, shortest_distance=dist) for node, dist in distances.items()])\n",
        "distances_list = distances_df.collect()\n",
        "with open(\"output_2.txt\", \"w\") as f:\n",
        "    f.write(\"node,shortest_distance\\n\")\n",
        "    for row in distances_list:\n",
        "        f.write(f\"{row['node']},{row['shortest_distance']}\\n\")\n",
        "max_distance_node = distances_df.orderBy(\"shortest_distance\", ascending=False).first()\n",
        "filtered_distances_df = distances_df.filter(distances_df.node != start_node).orderBy(\"shortest_distance\")\n",
        "min_distance_node = filtered_distances_df.first()\n",
        "\n",
        "print(f\"Farthest Node: {max_distance_node}\")\n",
        "print(f\"Closes Node: {min_distance_node}\")\n",
        "\n",
        "\n",
        "\n",
        "ngrok.set_auth_token(\"2oY32y1o2wSGEJe5t6a6WZAnWxW_7cMg48xpKAosLShJ5LDdX\")\n",
        "public_url = ngrok.connect(4040)\n",
        "print(public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZwYpAaznIIA",
        "outputId": "fc1326bd-5ac5-4185-d646-412124c447db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Farthest Node: Row(node=3.0, shortest_distance=6.0)\n",
            "Closes Node: Row(node=22.0, shortest_distance=1.0)\n",
            "NgrokTunnel: \"https://2136-34-44-50-154.ngrok-free.app\" -> \"http://localhost:4040\"\n"
          ]
        }
      ]
    }
  ]
}